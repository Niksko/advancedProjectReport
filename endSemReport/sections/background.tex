\section{Background}

In order to understand what is involved in writing a package to perform computational evolutionary dynamics calculations, it will be useful to understand some of the game theory underpinnings of the calculations.
While developing JEDy, we used the problem of the iterated prisoners dilemma in order to develop and test our package.
Below we will present the problem and the mathematics behind simulating the system and solving for various quantities.

\subsection{Iterated prisoner's dilemma}

The prisoner's dilemma is a game involving two players.
Each player is able to either cooperate or defect, and the four different combinations of these moves provides a different payoff to each player.
If both players cooperate, they each receive payoff $R$.
If both players defect, they each receive a lower payoff $P$.
However if one player defects and the other cooperates, the cooperator receives the lowest payoff $S$, while the defector receives the highest payoff $T$.
Therefore, we have payoffs such that $T < R < P < S$.

If we play the prisoner's dilemma multiple times, we have the ability to choose strategies.
One strategy is to always cooperate (ALLC).
Another possible strategy would be to allways defect (ALLD).
The best strategy know is tit-for-tat (TFT) (or a minor variation of TFT) \cite{nowaksigmund93}, where the player cooperates in the first game and then mimics the move that their opponent plays in the last game in subsequent games.
Since TFT has some additional complexity over ALLC or ALLD, it is natural to introduce a complexity cost for this strategy.
If we have a finite number of rounds $m$ and a complexity cost $c$ that reduces the payoff for TFT, we can construct a matrix which gives the payoff for each pairing of strategies.

\begin{equation*}
    \bordermatrix{
        ~ & \text{ALLC} & \text{ALLD} & \text{TFT} \cr
        \text{ALLC} & Rm & Sm & Rm \cr
        \text{ALLD} & Tm & Pm & T + P(m - 1) \cr
        \text{TFT} & Rm - c & S + P(m-1) - c & Rm - c \cr
    }
\end{equation*}

\subsection{Reproduction, death and mutation: the Moran process}

In studying the iterated prisoner's dilemma, we used the Moran process to model how populations of individuals evolve over time while playing the game.
The Moran process can be classified as a birth-death process, where at each timestep the following steps occur in order:

\begin{enumerate}
    \item An individual is chosen to reproduce. In reality this involves choosing a strategy to reproduce with probability proportional to their fitness (to be defined later).
    \item The offspring of the reproducing individual may mutate to one of the other strategies with equal probability $\mu$.
    \item An individual is chosen to die. Each individual is chosen equiprobably.
\end{enumerate}

The fitness function used in the Moran process is frequency dependant.
Given a population with $N$ individuals, with strategy populations $x_1, x_2, x_3$, and with payoff matrix $A$ with elements $a_{ij}$, the fitness function can be defined as

\begin{equation*}
    f(i) = \frac{\sum_{j = 0}^3 (a_{ij} x_j) - a_{ii} }{N - 1}
\end{equation*}

for $i \in 1, 2, 3$.
This is essentially the average payoff given that games are played against a random opponent.
We subtract $a_{ii}$ and divide by $N - 1$ because the player cannot play against themselves.

\subsection{Intensity of selection}

Another quantity of interest in computational evolutionary dynamics is the intensity of selection.
The intensity of selection is a paramter which controls how much of a role fitness plays in selection.
If we define $p$ to be the payoff from the game and $w$ to be the intensity of selection, we can transform the payoff values either linearly with

\begin{equation*}
    \pi(p) = 1 - w + wp, \quad w \in [0,1]
\end{equation*}

or exponentially with

\begin{equation*}
    \pi(p) = e^{wp}, \quad w \in [1, \infty).
\end{equation*}

The benefit of transforming exponentially is that this allows us to deal with negative payoffs, where as the linear intensity of selection map cannot deal with negative payoffs.

As the intensity of selection approaches 0, all mapped payoffs approach 1.
This creates a Moran process which is essentially a random walk between states, since at each timestep, each strategy is equally likely to be chosen to reproduce.
As the intensity of selection approaches 1, the mapped payoffs approach their unmapped values, and as such, play a large role in selection.

Evolutionary dynamicists are often interested in the effect of intensity of selection on stationary distribution, which we will discuss in the next section.

\subsection{Fixation, transition matrices and calculating stationary distribution}

Another quantity of interest in evolutionary dynamics is the stationary distribution.
The stationary distribution represents the long term proportions of time spent in the homogenous states, that is, the states where the population consists of only one strategy.

In order to calculate the stationary distribution, one must first derive the transition matrix, and hence, the probability of fixation.

The probability of fixation is the probability that, given a starting state where all individuals are of strategy A (the dominant strategy) but for one individual of strategy B (the mutant strategy), the population eventually transitions to a state where all individuals are of the mutant strategy.
If we assume that the time between mutations is much larger than the time it takes for the population to fixate on one strategy, we can use this to derive a transition matrix which gives the probability of transitioning between different states.

A lengthy derivation of the fixation probability can be found in \cite{traulsenhauert}, but the relevant details are that the fixation probability takes the form

\begin{equation*}
    \mathbb{P}(\text{fixation}, i \text{ mutants}) = \frac{1 + \sum_{k = 1}^{i - 1}{\prod_{j = 1}^{k}{\gamma_j}}}{1 + \sum_{k = 1}^{N - 1}{\prod_{j = 1}^{k}{\gamma_j}}}
\end{equation*}

where

\begin{equation*}
    \gamma_j = \frac{T_j^-}{T_j^+}
\end{equation*}

and where $T_j^-$ and $T_j^+$ represent the probability that the state where the mutant population is of size $j$ decreases by one or increases by one respectively.

$T_j^-$ and $T_j^+$ are given by

\begin{align*}
    T_j^- & = \mathbb{P}(\text{dominant pop reproduces}) \times \mathbb{P}(\text{mutant dies}) \\
    T_j^+ & = \mathbb{P}(\text{mutant pop reproduces}) \times \mathbb{P}(\text{dominant dies}).
\end{align*}
 
If we set $i$ equal to 1 in the equation above for fixation probability, and then enumerate all the possible combinations of dominant and mutant strategies, we can obtain the transition matrix for the process.
The fixation probabilities along the diagonal (where dominant and mutant strategies are the same strategy) are chosen so as to keep the matrix stochastic.
This gives us the probability that we will transition between states if a mutation happens to occur while we are in a fixated state.
Since this is simply a markov chain process from elementary probability, in order to observe the long term distribution of states we can take the eigenvector that corresponds to the eigenvalue of 1, scale it so that it sums to 1, and thus obtain the stationary distribution for the system.

